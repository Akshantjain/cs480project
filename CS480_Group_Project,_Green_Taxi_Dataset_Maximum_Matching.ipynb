{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3sU8ogfs49t"
   },
   "source": [
    "# CS480: Database Systems, Group Project\n",
    "### Green Taxi Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JTQY6FTCs49t"
   },
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime, timedelta\n",
    "import sqlite3\n",
    "from hopcroftkarp import HopcroftKarp\n",
    "import time\n",
    "from itertools import chain\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q69VAOVhs49t"
   },
   "source": [
    "#### 1. First, we will import the data in the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcPHfSCHs49t"
   },
   "outputs": [],
   "source": [
    "green = pd.read_csv('green_tripdata_2015_6months_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFrGFj__s49t"
   },
   "source": [
    "#### 2. Now, we will get the summary of the datasets and then we will clean up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkIHZO8ds49t"
   },
   "outputs": [],
   "source": [
    "green.describe()\n",
    "# print(green.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvncU07As49u"
   },
   "outputs": [],
   "source": [
    "# Getting the values which needs to be cleaned up before procedding further\n",
    "print(\"\\u0332\".join('Number of Null data values in each columns:'))\n",
    "print(green.isnull().sum())\n",
    "print('')\n",
    "print(\"\\u0332\".join('Number of Datasets:'), len(green.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHZXObJqs49u"
   },
   "outputs": [],
   "source": [
    "# Since the number of null values in the 'Ehail_fee' column is equal to the number of rows in the dataset\n",
    "# Therefore, we will drop the whole column due to its irrelevance.\n",
    "\n",
    "# green = green.drop(columns=['Ehail_fee'])\n",
    "# print(\"\\u0332\".join('Number of Null data values in each columns:'))\n",
    "# print(green.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZPb59X2ks49u"
   },
   "outputs": [],
   "source": [
    "# Now we will boxplot the 'Trip_type' column\n",
    "# boxplot = green.boxplot(column=['Trip_type '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zg3zZ21Is49u"
   },
   "outputs": [],
   "source": [
    "# In the boxplot, we can see that '2.0' is an outlier, therefore majority of the dataset have value '1'\n",
    "# So, we will replace the null values with '1'\n",
    "# green['Trip_type '] = green['Trip_type '].fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_W5PcaLZs49u",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now there are no null values in the dataset\n",
    "# Update the columns variable according to the new cleaned up value\n",
    "# columns = green.columns\n",
    "# print(\"\\u0332\".join('Number of Null data values in each columns:'))\n",
    "# print(green.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnYlecTLs49u"
   },
   "source": [
    "#### 3. Now, filter out the data and clean it up again and to get the data we need use for our algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KeI94stJs49u"
   },
   "outputs": [],
   "source": [
    "dataset = green[['VendorID', 'lpep_pickup_datetime', 'Lpep_dropoff_datetime', 'Pickup_longitude', 'Pickup_latitude', 'Dropoff_longitude', 'Dropoff_latitude', 'Passenger_count', 'Trip_distance']]\n",
    "\n",
    "# Here we are storing all the column names in a array named 'columns'\n",
    "columns = dataset.columns\n",
    "print(\"\\u0332\".join('Column Names:'), columns)\n",
    "\n",
    "print(\"\\u0332\".join('Dataset:'), len(dataset.index))\n",
    "dataset = dataset[ (dataset.lpep_pickup_datetime != dataset.Lpep_dropoff_datetime) & (dataset.Trip_distance != 0) & (dataset.Passenger_count < 3) & (dataset['Trip_distance'] != dataset['Trip_distance'].max())]\n",
    "\n",
    "print(\"\\u0332\".join('Filtered Dataset:'), len(dataset.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4tJtrjWfs49u",
    "outputId": "4bbb0877-ede0-40bd-9b38-234501d1bfd6"
   },
   "outputs": [],
   "source": [
    "print(\"\\u0332\".join('Total Distance Travelled:'), dataset['Trip_distance'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "My95kdzWs49v"
   },
   "outputs": [],
   "source": [
    "# Correcting the date and time format\n",
    "dataset['lpep_pickup_datetime'] = pd.to_datetime(dataset['lpep_pickup_datetime'])\n",
    "dataset['Lpep_dropoff_datetime'] = pd.to_datetime(dataset['Lpep_dropoff_datetime'])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXr8yPWGs49v"
   },
   "source": [
    "#### 4. Now use SQL Queries to compute the average speed of each trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkPegUDUs49v"
   },
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect('TestDB1.db')\n",
    "# c = conn.cursor()\n",
    "\n",
    "# dataset.to_sql('Green', conn, if_exists='replace', index = False)\n",
    " \n",
    "# c.execute('''  \n",
    "# SELECT COUNT(*) FROM Green\n",
    "#           ''')\n",
    "\n",
    "# print(c.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ji9MBdXKs49v"
   },
   "outputs": [],
   "source": [
    "from math import radians, cos, sin, asin, sqrt \n",
    "def distance(lat1, lon1, lat2, lon2): \n",
    "    \n",
    "#     start = time.time()\n",
    "    \n",
    "    # The math module contains a function named \n",
    "    # radians which converts from degrees to radians. \n",
    "    lon1 = radians(lon1) \n",
    "    lon2 = radians(lon2) \n",
    "    lat1 = radians(lat1) \n",
    "    lat2 = radians(lat2) \n",
    "       \n",
    "    # Haversine formula  \n",
    "    dlon = lon2 - lon1  \n",
    "    dlat = lat2 - lat1 \n",
    "\n",
    "    c = (3956 * 2 * asin(sqrt(sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2)))\n",
    "\n",
    "#     end = time.time()\n",
    "#     print(end-start)\n",
    "    \n",
    "    return c\n",
    "     \n",
    "    # Radius of earth in kilometers. Use 3956 for miles \n",
    "    # calculate the result \n",
    "    # (c * r) \n",
    "\n",
    "def time_difference(time1, time2):\n",
    "    time_diff = (time2 - time1)\n",
    "    time_seconds = abs(time_diff.total_seconds())\n",
    "    \n",
    "    return time_seconds / 60.0\n",
    "\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jtz7stE_s49v"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.sort_values(by=['lpep_pickup_datetime', 'Lpep_dropoff_datetime'])\n",
    "# dataset = dataset.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hB2jw_Uks49v"
   },
   "outputs": [],
   "source": [
    "dataset['Average Speed (MPH)'] = dataset.apply(lambda row : row.Trip_distance / (time_difference(row.lpep_pickup_datetime, row.Lpep_dropoff_datetime) / 60.0), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44AtuaDys49v"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mK0YYIwos49v"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbYPZKass49v"
   },
   "outputs": [],
   "source": [
    "def mergeable_check(trip1, trip2, delay):\n",
    "    \n",
    "#     avg_speed = (trip1['Average Speed (MPH)'] + trip2['Average Speed (MPH)'])/2\n",
    "    \n",
    "    Do1o2 = distance(trip1['Pickup_latitude'], trip1['Pickup_longitude'], trip2['Pickup_latitude'], trip2['Pickup_longitude'])\n",
    "    Dd1d2 = distance(trip1['Dropoff_latitude'], trip1['Dropoff_longitude'], trip2['Dropoff_latitude'], trip2['Dropoff_longitude'])\n",
    "\n",
    "    Do2d1 = distance(trip2['Pickup_latitude'], trip2['Pickup_longitude'], trip1['Dropoff_latitude'], trip1['Dropoff_longitude'])\n",
    "    Do2d2 = distance(trip2['Pickup_latitude'], trip2['Pickup_longitude'], trip2['Dropoff_latitude'], trip2['Dropoff_longitude'])\n",
    "    \n",
    "#     To1o2 = Do1o2/avg_speed\n",
    "#     Td1d2 = Dd1d2/avg_speed\n",
    "    \n",
    "#     To2d1 = Do2d1/avg_speed\n",
    "#     To2d2 = Do2d2/avg_speed\n",
    "    \n",
    "#     print(max([Do2d1, Do2d2]) + Do1o2 + Dd1d2, \"|\", trip1['Trip_distance'] + trip2['Trip_distance'])\n",
    "    \n",
    "#     sequence1 = False\n",
    "#     sequence2 = False\n",
    "    \n",
    "#     sequence1Time = Trip1['lpep_pickup_datetime']\n",
    "#     sequence2Time = Trip2['lpep_pickup_datetime']\n",
    "    \n",
    "    dist = max([Do2d1, Do2d2]) + Do1o2 + Dd1d2\n",
    "    if (dist < trip1['Trip_distance'] + trip2['Trip_distance']):\n",
    "#         print('True', dist, trip1['Trip_distance'] + trip2['Trip_distance'])\n",
    "        return (True, (trip1['Trip_distance'] + trip2['Trip_distance']) - dist)\n",
    "    else:\n",
    "#         print('False', dist, trip1['Trip_distance'] + trip2['Trip_distance'])\n",
    "        return (False, 0)\n",
    "    \n",
    "#     if (sequence1 or sequence2): return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OM_Yf0R8s49v",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trips_processed = 0\n",
    "mergeable_trips = 0\n",
    "\n",
    "def shared_trips_eval(dataset, delay, length):    \n",
    "    \n",
    "    global trips_processed \n",
    "    global mergeable_trips\n",
    "    \n",
    "    rides_dict = {}\n",
    "    rides = {}\n",
    "    \n",
    "    for index1 in range(length):\n",
    "        index2 = index1 + 1\n",
    "        Trip1 = dataset.iloc[index1]\n",
    "        \n",
    "        trips_processed += 1\n",
    "        count = 0\n",
    "        while (index2 < length):\n",
    "            count += 1\n",
    "            Trip2 = dataset.iloc[index2]\n",
    "            if (Trip1['Passenger_count'] + Trip2['Passenger_count'] <= 3):\n",
    "                if (time_difference(Trip1['lpep_pickup_datetime'], Trip2['lpep_pickup_datetime']) > delay or time_difference(Trip1['Lpep_dropoff_datetime'], Trip2['Lpep_dropoff_datetime']) > delay):\n",
    "#                     print(\"Break, \", count)\n",
    "                    break\n",
    "            \n",
    "            mergeable, dist = mergeable_check(Trip1, Trip2, delay)\n",
    "            if (mergeable):\n",
    "                rides.update({index1: {dist}})\n",
    "                mergeable_trips += 1\n",
    "            index2 = index2 + 1\n",
    "        print(index1)\n",
    "#         print('')\n",
    "#         if (index1 % 200 == 0):\n",
    "#             print(index1)\n",
    "    return rides\n",
    "\n",
    "start = time.time()\n",
    "merged_trips = shared_trips_eval(dataset[:100000], 20.0, len(dataset[:100000]))\n",
    "end = time.time()\n",
    "# end = time.time()\n",
    "# print('')\n",
    "# print('Trips processed: ', trips_processed, 'Time Taken: ' ,end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9G-CMWNTs49v"
   },
   "outputs": [],
   "source": [
    "print(merged_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Kwfcylus49v"
   },
   "outputs": [],
   "source": [
    "max_matched = HopcroftKarp(merged_trips).maximum_matching(keys_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wd__9DTQ6Y_b",
    "outputId": "47bb75fc-12fb-4d1e-8d1e-6eb978b44def"
   },
   "outputs": [],
   "source": [
    "print(sum(max_matched.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nbjuA9Q5s49v",
    "outputId": "713ebe4a-679d-4d77-b224-3ee1b0884d07"
   },
   "outputs": [],
   "source": [
    "print('')\n",
    "print('Trips processed: ', trips_processed) \n",
    "print('MergeAble Trips: ', len(max_matched)) \n",
    "print('Time Taken to process: ' ,end-start)\n",
    "print('Total Distance Travelled:', dataset[:100000]['Trip_distance'].sum())\n",
    "print('Total Miles saved: ', sum(max_matched.values()))\n",
    "#print('Last Trip Processed: ')\n",
    "#print(dataset.iloc[trips_processed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CS480 - Group Project, Green Taxi Dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
